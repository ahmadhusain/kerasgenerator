---
title: "Data Generator for Time Series Models"
author: "R. Dimas Bagas Herlambang"
date: "Last updated on: `r format(Sys.Date(), '%B %e, %Y')`"
bibliography: timeseries.bib
csl: apa.csl
link-citations: yes
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Data Generator for Time Series Model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}

# chunk opts
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  out.width = "100%"
)

```

Preparing a proper dataset for a supervised time series model in R sometimes could become very complex and tedious. There are some tutorials that already cover this kind of task: check out the tutorial made by [Dancho and Keydana](https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/) [-@dancho2018predicting] for basic univariate model fitting without using data generator, and the one made by [Chollet and Allaire](https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/) [-@chollet2017time] for the multivariate model with a custom data generator. However, as you could see from the articles, there are no general framework on how to properly do the data preparation process.

In this article we will show you how `kerasgenerator` could help us in preparing a data generator for supervised time series [`keras`](https://keras.rstudio.com) models.

# Understanding the Time Series Array Shape {#introduction}

To fit a supervised time series model, many of `keras`' functions expect the target and feature data to be in an `array` object with a specific number of dimension. The input and output `array` need to have at least three dimension that representing:

1. Observation index
2. Number of timesteps
3. Number of distinct feature / target

To understand better what each dimensions representing, let's follow a quick example using the popular [Internet Traffic Data from 11 European Cities](https://datamarket.com/data/set/232n/internet-traffic-data-in-bits-from-a-private-isp-with-centres-in-11-european-cities-the-data-corresponds-to-a-transatlantic-link-and-was-collected-from-0657-hours-on-7-june-to-1117-hours-on-31-july-2005-data-collected-at-five-minute-intervals#!ds=232n&display=line) dataset, provided by Time Series Data Library [@hyndman@2019time]:

```{r}

# import libs
library(magrittr)
library(tidyverse)

# import dataset
traffic_tbl <- read_csv(
  file = "data/internet-traffic-data-in-bits-fr.csv",
  col_names = c("datetime", "traffic"),
  skip = 1
)

# delete last row which contain NA
traffic_tbl %<>% slice(-nrow(.))

# quick check
glimpse(traffic_tbl)

```

The dataset is in a general time series format--it contains a time identifier column, each row representing a timestep, and it is regularly ordered. Let's see the time series pattern for the last 7 days in the data:

```{r}

# import libs
library(scales)
library(tidyquant)

# plot the last 7 days
traffic_tbl %>%
  tail(12 * 24 * 7) %>%
  ggplot(aes(x = datetime, y = traffic)) +
    geom_line() +
    labs(x = NULL, y = NULL) +
    scale_x_datetime(label = date_format("%a, %b %e, %H:%M:%S")) +
    scale_y_continuous(
      label = unit_format(unit = "M", scale = 1e-09, accuracy = 0.01)
    ) +
    theme_tq()

```

In supervised time series model, we need to define two important parameters for our datasets. The first parameter is the **lookback period**. The lookback period is determining **how many period behind should the targets look-up for its signal**, but it should be noted that **the periods in-between will be skipped**. The second parameter is the **timestep length**. This parameter define **the length of an observation of features** that would be considered as a sequence of signal for the target. **Note** that the terms that I just mentioned might be differ from other source, but I really suggest you to read the original paper of [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) layer [@hochreiter1997long] and other valid sources to reach a better understanding of the terminologies.

Now let's make some illustration using our dataset to make it clear. Supposed that we want to predict the value of 5-minutes total traffic using the traffic pattern in the previous hour. If, we pick the last observation of our dataset as our target, then the target and features will be:

```{r}

# import libs
library(lubridate)

# set some parameters
lookback <- 12
timesteps <- 12

# specify the target and feature
target <- tail(traffic_tbl$datetime, 1)
feature_end <- target - 5 * minutes(lookback)
feature_start <- feature_end - 5 * minutes(timesteps) + minutes(5)

```

Notice that I set the `feature_start` to be `feature_end - timesteps + 1`--the `+ 1` is represented by `minutes(5)` in this case. This is because the `timesteps` value is representing how long is our feature--without `+ 1`, the timesteps would be longer by 1 value. See the following figure for an illustration:

```{r, echo=FALSE, fig.cap="Supervised time series model illustration"}

# illustration
traffic_tbl %>%
  tail(12 * 2.5) %>%
  ggplot(aes(x = datetime, y = traffic)) +
    geom_line() +
    geom_point(
      data = traffic_tbl %>%
        filter(datetime %in% c(feature_start, feature_end, target)) %>%
        mutate(id = c("Feature", "Feature", "Target")),
      aes(x = datetime, y = traffic, colour = id)
    ) +
    annotate(
      geom = "text",
      x = target,
      y = 7.10 * 1e+09,
      label = "Target",
      colour = "darkred",
      angle = 90
    ) +
    annotate(
      geom = "segment",
      x = target,
      xend = target,
      y = 7 * 1e+09,
      yend = 6.75 * 1e+09,
      colour = "darkred",
      arrow = arrow(length = unit(0.25, "cm"))
    ) +
    annotate(
      geom = "text",
      x = feature_start + difftime(feature_end, feature_start) / 2,
      y = 7.10 * 1e+09,
      label = glue::glue(
        "A sequence feature", "\n",
        "with {timesteps} timesteps"
      ),
      colour = "darkblue"
    ) +
    annotate(
      geom = "rect",
      xmin = feature_start,
      xmax = feature_end,
      ymin = -Inf,
      ymax = Inf,
      alpha = 0.1,
      fill = "darkblue"
    ) +
    labs(x = NULL, y = NULL) +
    guides(colour = FALSE) +
    theme_tq() +
    scale_x_datetime(label = date_format("%b %e, %H:%M:%S")) +
    scale_y_continuous(
      label = unit_format(unit = "M", scale = 1e-09, accuracy = 0.01)
    ) +
    scale_colour_manual(
      values = c("Feature" = "darkblue", "Target" = "darkred"),
      breaks = c("Feature", "Target")
    )

```

If we follow this format, then we could convert the data into `array` matrices like this:

```{r}

# container arrays
x_array <- array(0, dim = c(1, timesteps, 1))
y_array <- array(0, dim = c(1, 1))

# specify the row indices
y_row <- nrow(traffic_tbl)
x_row <- y_row - lookback

# adjust the x indices according to the timesteps
x_indices <- seq(
  from = x_row - timesteps + 1,
  to = x_row
)

# convert the table into matrix
traffic_matrix <- data.matrix(traffic_tbl)

# fill the arrays
x_array[1, , 1] <- traffic_matrix[x_indices, 2]
y_array[1, 1] <- traffic_matrix[y_row, 2]

```

Let's confirm the structure and content inside the arrays:

```{r}

# check the structure
str(x_array)
str(y_array)

```

Basically, the [`series_generator()`](/reference/series_generator) function is transforming your data into the format similar to above explanation; [`forecast_generator()`]((/reference/forecast_generator)) also works in a similar way, but disregards any attributes related to `y` (see [forecasting section](#forecast) for further explanation). Let's take a look on how those functions in action in the following sections.

# Fitting a Time Series Model using Data Generator {#fitting}

In this example, we will continue using the Internet Traffic dataset to fit a time series Keras model using `kerasgenerator`.

To build a time series data generator, we need to specify some parameters:

```{r}

# the x and y index
x <- "traffic"
y <- "traffic"

# supervised parameter
lookback <- 12
timesteps <- 12

# dataset settings
train_length <- 12 * 24 * 7
val_length <- 12 * 24
batch_size <- 12 * 24

# train-val row indices
val_end <- nrow(traffic_tbl)
val_start <- val_end - val_length + 1

train_end <- val_start - 1
train_start <- train_end - train_length + 1

# number of steps to see full data
train_steps <- ceiling(train_length / batch_size)
val_steps <- ceiling(val_length / batch_size)

```

Before we continue, let's define a custom function for data preprocessing on the fly. We will use [`recipes`](https://tidymodels.github.io/recipes/) functionality within our custom function:

```{r}

# import libs
library(recipes)

# recipe: square root, center, scale
recipe_obj <- recipe(traffic ~ ., traffic_tbl[train_start:train_end, ]) %>%
  step_sqrt(all_outcomes()) %>%
  step_center(all_outcomes()) %>%
  step_scale(all_outcomes()) %>%
  prep()

# custom preprocess function
prep_funs <- function(data) {
  
  # preprocess
  newdata <- bake(
    object = recipe_obj,
    new_data = data
  )
  # return the data
  return(newdata)
  
}

```

Finally, we could start to make generators for the train and validation data:

```{r}

# import libs
library(kerasgenerator)

# data generator
train_gen <- series_generator(
  data = traffic_tbl,
  y = y,
  x = x,
  lookback = lookback,
  timesteps = timesteps,
  start_index = train_start,
  end_index = train_end,
  batch_size = batch_size,
  return_target = TRUE,
  prep_funs = prep_funs
)

val_gen <- series_generator(
  data = traffic_tbl,
  y = y,
  x = x,
  lookback = lookback,
  timesteps = timesteps,
  start_index = val_start,
  end_index = val_end,
  batch_size = batch_size,
  return_target = TRUE,
  prep_funs = prep_funs
)

```

Now we can proceed to define our time series Keras model:

```{r}

# import libs
library(keras)

# initiate a sequential model
model <- keras_model_sequential()

# define the model
model %>%
  
  # layer lstm
  layer_lstm(
    name = "lstm1",
    input_shape = list(timesteps, length(x)),
    units = 128,
    dropout = 0.1,
    recurrent_dropout = 0.1,
    return_sequences = TRUE
  ) %>%
  
  layer_lstm(
    name = "lstm2",
    units = 64,
    dropout = 0.1,
    recurrent_dropout = 0.1,
    return_sequences = TRUE
  ) %>%
  
  layer_lstm(
    name = "lstm3",
    units = 32,
    dropout = 0.1,
    recurrent_dropout = 0.1,
    return_sequences = FALSE
  ) %>%
  
  # layer output
  layer_dense(
    name = "output",
    units = length(y)
  )

# compile the model
model %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)

# model summary
summary(model)

```

To fit this model, we will use [`fit_generator()`]((https://keras.rstudio.com/reference/fit_generator.html)) function:

```{r}

# set number of epochs
epochs <- 100

# model fitting
history <- model %>% fit_generator(
  generator = train_gen,
  steps_per_epoch = train_steps,
  epochs = epochs,
  validation_data = val_gen,
  validation_steps = val_steps
)

# history plot
plot(history)

```

You can also pass the generator to [`evaluate_generator()`]((https://keras.rstudio.com/reference/evaluate_generator.html)) to evaluate the results:

```{r}

# evaluate on validation dataset
model %>% evaluate_generator(
  generator = val_gen,
  steps = val_steps
)

```

# Forecasting using Data Generator {#forecast}

Like in [`series_generator()`](/reference/series_generator) function, we need to specify some parameters for [`forecast_generator()`](/reference/forecast_generator):

```{r}

# forecast horizon
horizon <- 12

# forecast indices
last_index <- nrow(traffic_tbl)

# number of steps to see full data
fcast_steps <- ceiling(horizon / batch_size)

```

Note that the **forecast `horizon` should not be greater than the value of `lookback` if you want to forecast from the last observation**.

Then, we can proceed to define the generator and pass it to [`predict_generator()`]((https://keras.rstudio.com/reference/predict_generator.html)) function:

```{r}

# forecast generator
fcast_gen <- forecast_generator(
  data = traffic_tbl,
  x = x,
  lookback = lookback,
  timesteps = timesteps,
  last_index = last_index,
  horizon = horizon,
  batch_size = batch_size
)

# forecast using generator
fcast <- model %>% predict_generator(
  generator = fcast_gen,
  steps = fcast_steps
)

# check the structure
str(fcast)

```

# References {#reference}
